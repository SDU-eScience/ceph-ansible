---
# ceph-ansible all.yml
# Claudio Pica

###########
# GENERAL #
###########

######################################
# Releases name to number dictionary #
######################################
ceph_release_num:
  luminous: 12
  mimic: 13
  nautilus: 14
  octopus: 15

# Directory to fetch cluster fsid, keys etc...
fetch_directory: fetch/

# The 'cluster' variable determines the name of the cluster.
# Changing the default value to something else means that you will
# need to change all the command line calls as well, for example if
# your cluster name is 'foo':
# "ceph health" will become "ceph --cluster foo health"
#
# An easier way to handle this is to use the environment variable CEPH_ARGS
# So run: "export CEPH_ARGS="--cluster foo"
# With that you will be able to run "ceph health" normally
cluster: ceph

# Inventory host group variables
mon_group_name: mons
osd_group_name: osds
#rgw_group_name: rgws
mds_group_name: mdss
#nfs_group_name: nfss
#rbdmirror_group_name: rbdmirrors
client_group_name: clients
#iscsi_gw_group_name: iscsigws
mgr_group_name: mgrs

# If configure_firewall is true, then ansible will try to configure the
# appropriate firewalling rules so that Ceph daemons can communicate
# with each others.
configure_firewall: false

# Open ports on corresponding nodes if firewall is installed on it
#ceph_mon_firewall_zone: public
#ceph_mgr_firewall_zone: public
#ceph_osd_firewall_zone: public
#ceph_rgw_firewall_zone: public
#ceph_mds_firewall_zone: public
#ceph_nfs_firewall_zone: public
#ceph_rbdmirror_firewall_zone: public
#ceph_iscsi_firewall_zone: public
#ceph_dashboard_firewall_zone: public
#ceph_rgwloadbalancer_firewall_zone: public

# Generate local ceph.conf in fetch directory
#ceph_conf_local: false

############
# PACKAGES #
############
#debian_package_dependencies:
#  - python-pycurl

centos_package_dependencies:
  - python-pycurl
  - python-setuptools
  - libselinux-python
  - smartmontools
  - python36-yaml
  - python36-six

#redhat_package_dependencies:
#  - python-pycurl
#  - python-setuptools

#suse_package_dependencies:
#  - python-pycurl
#  - python-xml
#  - python-setuptools

# Whether or not to install the ceph-test package.
ceph_test: true

# Enable the ntp service by default to avoid clock skew on ceph nodes
# Disable if an appropriate NTP client is already installed and configured
ntp_service_enabled: false # we already have it configured

# Set type of NTP client daemon to use, valid entries are chronyd, ntpd or timesyncd
# Note that this selection is currently ignored on containerized deployments
ntp_daemon_type: chronyd


# Set uid/gid to default '64045' for bootstrap directories.
# '64045' is used for debian based distros. It must be set to 167 in case of rhel based distros.
# These values have to be set according to the base OS used by the container image, NOT the host.
#bootstrap_dirs_owner: "167"
#bootstrap_dirs_group: "167"

# This variable determines if ceph packages can be updated.  If False, the
# package resources will use "state=present".  If True, they will use
# "state=latest".
upgrade_ceph_packages: False

#ceph_use_distro_backports: false # DEBIAN ONLY
#ceph_directories_mode: "0755"


###########
# INSTALL #
###########
ceph_origin: distro


######################
# CEPH CONFIGURATION #
######################

## Ceph options
#
# Each cluster requires a unique, consistent filesystem ID. By
# default, the playbook generates one for you and stores it in a file
# in `fetch_directory`. If you want to customize how the fsid is
# generated, you may find it useful to disable fsid generation to
# avoid cluttering up your ansible repo. If you set `generate_fsid` to
# false, you *must* generate `fsid` in another way.
# ACTIVATE THE FSID VARIABLE FOR NON-VAGRANT DEPLOYMENT
fsid: "{{ cluster_uuid.stdout }}"
generate_fsid: True

ceph_conf_key_directory: /etc/ceph

#ceph_uid: 167

# Permissions for keyring files in /etc/ceph
ceph_keyring_permissions: '0600'

cephx: true

## Client options
#
rbd_cache: "true"
rbd_cache_writethrough_until_flush: "true"
rbd_concurrent_management_ops: 20

rbd_client_directories: true # this will create rbd_client_log_path and rbd_client_admin_socket_path directories with proper permissions

# Permissions for the rbd_client_log_path and
# rbd_client_admin_socket_path. Depending on your use case for Ceph
# you may want to change these values. The default, which is used if
# any of the variables are unset or set to a false value (like `null`
# or `false`) is to automatically determine what is appropriate for
# the Ceph version with non-OpenStack workloads -- ceph:ceph and 0770
# for infernalis releases, and root:root and 1777 for pre-infernalis
# releases.
# If you set rbd_client_directory_mode, you must use a string (e.g.,
# 'rbd_client_directory_mode: "0755"', *not*
# 'rbd_client_directory_mode: 0755', or Ansible will complain: mode
# must be in octal or symbolic form
rbd_client_directory_owner: null
rbd_client_directory_group: null
rbd_client_directory_mode: null

rbd_client_log_path: /var/log/ceph
rbd_client_log_file: "{{ rbd_client_log_path }}/qemu-guest-$pid.log" # must be writable by QEMU and allowed by SELinux or AppArmor
rbd_client_admin_socket_path: /var/run/ceph # must be writable by QEMU and allowed by SELinux or AppArmor

## Monitor options
#
# You must define either monitor_interface, monitor_address or monitor_address_block.
# These variables must be defined at least in all.yml and overrided if needed (inventory host file or group_vars/*.yml).
# Eg. If you want to specify for each monitor which address the monitor will bind to you can set it in your **inventory host file** by using 'monitor_address' variable.
# Preference will go to monitor_address if both monitor_address and monitor_interface are defined.
#monitor_interface: interface
#monitor_address: 0.0.0.0
#monitor_address_block: subnet
## we set the ip as host variable
# set to either ipv4 or ipv6, whichever your network is using
ip_version: ipv4
#mon_use_fqdn: false # if set to true, the MON name used will be the fqdn in the ceph.conf

##########
# CEPHFS #
##########
osd_pool_default_size: 3
osd_pool_default_min_size: 1
osd_pool_default_pg_num: 128

### DANGEROUS
ceph_remove_pools:
  # - "kube"
  # - "escience"
  # - "cephfs_data"
  # - "cephfs_metadata"



ceph_pools_replicated:
  - name: cephfs_metadata
    pgs: 512
    size: 4
    min_size: 2
    target_size_ratio: 0.05
    applications:
      - cephfs
  - name: kube
    pgs: 512
    size: 4
    min_size: 2
    target_size_ratio: 0.10
    applications:
      - rbd
  - name: escience
    pgs: 512
    size: 4
    min_size: 2
    target_size_ratio: 0.10
    applications:
      - rbd

ceph_ec_profiles:
  - name: ec_83h
    k: 8
    m: 3
    plugin: jerasure
    technique: reed_sol_van
    crush_failure_domain: host

ceph_pools_ec:
  - name: cephfs_data
    pgs: 1024
    ec_profile: ec_83h
    target_size_ratio: 0.4
    applications:
      - cephfs
  # - name: test_ec
  #   pgs: 512
  #   ec_profile: ec_83h
  #   target_size_ratio: 0.2
  #   applications:
  #     - rados


create_cephfs: false
cephfs: cephfs # name of the ceph filesystem
cephfs_data: cephfs_data # name of the data pool for a given filesystem
cephfs_metadata: cephfs_metadata # name of the metadata pool for a given filesystem
cephfs_pool_create: False

# these were created by hand
# cephfs_pools:
#   - { name: "{{ cephfs_data }}", pgs: "{{ osd_pool_default_pg_num }}", size: "{{ osd_pool_default_size }}", min_size: "{{ osd_pool_default_min_size }}"}
#   - { name: "{{ cephfs_metadata }}", pgs: "{{ osd_pool_default_pg_num }}", size: "{{ osd_pool_default_size }}", min_size: "{{ osd_pool_default_min_size }}"}

## OSD options
#
is_hci: false
#hci_safety_factor: 0.2
non_hci_safety_factor: 0.7
osd_memory_target: 4294967296
journal_size: 5120 # OSD journal size in MB
block_db_size: -1 # block db size in bytes for the ceph-volume lvm batch. -1 means use the default of 'as big as possible'.
public_network: "172.26.0.0/16"
cluster_network: "172.27.0.0/16"
osd_mkfs_type: xfs
osd_mkfs_options_xfs: -f -i size=2048
osd_mount_options_xfs: noatime,largeio,inode64,swalloc
osd_objectstore: bluestore

# Any device containing these patterns in their path will be excluded.
osd_auto_discovery_exclude: "dm-*|loop*"


## MDS options
#
#mds_use_fqdn: false # if set to true, the MDS name used will be the fqdn in the ceph.conf
mds_max_mds: 2 # should be <=#mds - 1 for HA so that at least 1 mds is in standby mode

## Rados Gateway options
#
#radosgw_frontend_type: civetweb # For additionnal frontends see: http://docs.ceph.com/docs/mimic/radosgw/frontends/

#radosgw_civetweb_port: 8080
#radosgw_civetweb_num_threads: 100
#radosgw_civetweb_options: "num_threads={{ radosgw_civetweb_num_threads }}"
# For additional civetweb configuration options available such as SSL, logging,
# keepalive, and timeout settings, please see the civetweb docs at
# https://github.com/civetweb/civetweb/blob/master/docs/UserManual.md

#radosgw_frontend_port: "{{ radosgw_civetweb_port if radosgw_frontend_type == 'civetweb' else '8080' }}"
#radosgw_frontend_options: "{{ radosgw_civetweb_options if radosgw_frontend_type == 'civetweb' }}"


# You must define either radosgw_interface, radosgw_address.
# These variables must be defined at least in all.yml and overrided if needed (inventory host file or group_vars/*.yml).
# Eg. If you want to specify for each radosgw node which address the radosgw will bind to you can set it in your **inventory host file** by using 'radosgw_address' variable.
# Preference will go to radosgw_address if both radosgw_address and radosgw_interface are defined.
#radosgw_interface: interface
#radosgw_address: 0.0.0.0
#radosgw_address_block: subnet
#radosgw_keystone_ssl: false # activate this when using keystone PKI keys
#radosgw_num_instances: 1
# Rados Gateway options
#email_address: foo@bar.com



## Handlers - restarting daemons after a config change
# if for whatever reasons the content of your ceph configuration changes
# ceph daemons will be restarted as well. At the moment, we can not detect
# which config option changed so all the daemons will be restarted. Although
# this restart will be serialized for each node, in between a health check
# will be performed so we make sure we don't move to the next node until
# ceph is not healthy
# Obviously between the checks (for monitors to be in quorum and for osd's pgs
# to be clean) we have to wait. These retries and delays can be configurable
# for both monitors and osds.
#
# Monitor handler checks
handler_health_mon_check_retries: 5
handler_health_mon_check_delay: 10
#
# OSD handler checks
handler_health_osd_check_retries: 40
handler_health_osd_check_delay: 30
handler_health_osd_check: true
#
# MDS handler checks
handler_health_mds_check_retries: 5
handler_health_mds_check_delay: 10
#
# RGW handler checks
#handler_health_rgw_check_retries: 5
#handler_health_rgw_check_delay: 10

# NFS handler checks
#handler_health_nfs_check_retries: 5
#handler_health_nfs_check_delay: 10

# RBD MIRROR handler checks
#handler_health_rbd_mirror_check_retries: 5
#handler_health_rbd_mirror_check_delay: 10

# MGR handler checks
handler_health_mgr_check_retries: 5
handler_health_mgr_check_delay: 10

## health mon/osds check retries/delay:
health_mon_check_retries: 20
health_mon_check_delay: 10
health_osd_check_retries: 20
health_osd_check_delay: 10

###############
# NFS-GANESHA #
###############

# Confiure the type of NFS gatway access.  At least one must be enabled for an
# NFS role to be useful
#
# Set this to true to enable File access via NFS.  Requires an MDS role.
#nfs_file_gw: false
# Set this to true to enable Object access via NFS. Requires an RGW role.
#nfs_obj_gw: true


#############
# MULTISITE #
#############

rgw_multisite: false

# The following Multi-site related variables should be set by the user.
# rgw_zone is set to "default" to enable compression for clusters configured without rgw multi-site
# If multisite is configured rgw_zone should not be set to "default". See README-MULTISITE.md for an example.
#rgw_zone: default

#rgw_zonemaster: true
#rgw_zonesecondary: false
#rgw_multisite_endpoint_addr: "{{ ansible_fqdn }}"
#rgw_zonegroup: solarsystem # should be set by the user
#rgw_zone_user: zone.user
#rgw_realm: milkyway # should be set by the user
#system_access_key: 6kWkikvapSnHyE22P7nO # should be re-created by the user
#system_secret_key: MGecsMrWtKZgngOHZdrd6d3JxGO5CPWgT2lcnpSt # should be re-created by the user

# Multi-site remote pull URL variables
#rgw_pull_port: "{{ radosgw_civetweb_port }}"
#rgw_pull_proto: "http"
#rgw_pullhost: localhost # rgw_pullhost only needs to be declared if there is a zone secondary. It should be the same as rgw_multisite_endpoint_addr for the master cluster


###################
# CONFIG OVERRIDE #
###################

# Ceph configuration file override.
# This allows you to specify more configuration options
# using an INI style format.
#
# When configuring RGWs, make sure you use the form [client.rgw.*]
# instead of [client.radosgw.*].
# For more examples check the profiles directory of https://github.com/ceph/ceph-ansible.
#
# The following sections are supported: [global], [mon], [osd], [mds], [rgw]
#
# Example:
# ceph_conf_overrides:
#   global:
#     foo: 1234
#     bar: 5678
#   "client.rgw.{{ hostvars[groups.get(rgw_group_name)[0]]['ansible_hostname'] }}":
#     rgw_zone: zone1
#
ceph_conf_overrides:
  global:
    mon_target_pg_per_osd: 100
    osd_pool_default_pg_autoscale_mode: "warn"
    ms_bind_prefer_ipv4: true
    # ms_cluster_type: "async+rdma"
    # ms_async_rdma_device_name: "mlx5_0,mlx5_1"
    # ms_async_rdma_cm: "true"

timeout_command: "timeout --foreground -s KILL 10s"

#############
# OS TUNING #
#############

disable_transparent_hugepage: "{{ false if osd_objectstore == 'bluestore' else true }}"
os_tuning_params:
  - { name: fs.file-max, value: 26234859 }
  - { name: vm.zone_reclaim_mode, value: 0 }
  - { name: vm.swappiness, value: 10 }
  - { name: vm.min_free_kbytes, value: "{{ vm_min_free_kbytes }}" }

# For Debian & Red Hat/CentOS installs set TCMALLOC_MAX_TOTAL_THREAD_CACHE_BYTES
# Set this to a byte value (e.g. 134217728 = 128MB)
# A value of 0 will leave the package default.
ceph_tcmalloc_max_total_thread_cache: 134217728


##########
# DOCKER #
##########
#ceph_docker_image: "ceph/daemon"
#ceph_docker_image_tag: latest
#ceph_docker_registry: docker.io
#ceph_docker_registry_auth: false
#ceph_docker_registry_username:
#ceph_docker_registry_password:
## Client only docker image - defaults to {{ ceph_docker_image }}
#ceph_client_docker_image: "{{ ceph_docker_image }}"
#ceph_client_docker_image_tag: "{{ ceph_docker_image_tag }}"
#ceph_client_docker_registry: "{{ ceph_docker_registry }}"
#ceph_docker_enable_centos_extra_repo: false
#ceph_docker_on_openstack: false
containerized_deployment: False
#container_binary:
#timeout_command: "{{ 'timeout --foreground -s KILL ' ~ docker_pull_timeout if (docker_pull_timeout != '0') and (ceph_docker_dev_image is undefined or not ceph_docker_dev_image) else '' }}"

# this is only here for usage with the rolling_update.yml playbook
# do not ever change this here
#rolling_update: false

#####################
# Docker pull retry #
#####################
#docker_pull_retry: 3
#docker_pull_timeout: "300s"


#############
# OPENSTACK #
#############
openstack_config: false


#############
# DASHBOARD #
#############
#dashboard_enabled: True
# Choose http or https
# For https, you should set dashboard.crt/key and grafana.crt/key
# If you define the dashboard_crt and dashboard_key variables, but leave them as '',
# then we will autogenerate a cert and keyfile
#dashboard_protocol: http
#dashboard_port: 8443
dashboard_admin_user: hansen
#dashboard_admin_user_ro: false
# This variable must be set with a strong custom password when dashboard_enabled is True
dashboard_admin_password: CHANGE_THIS!!
# We only need this for SSL (https) connections
#dashboard_crt: ''
#dashboard_key: ''
#dashboard_tls_external: false
#dashboard_grafana_api_no_ssl_verify: False
#dashboard_rgw_api_user_id: ceph-dashboard
#dashboard_rgw_api_admin_resource: ''
#dashboard_rgw_api_no_ssl_verify: False
#dashboard_frontend_vip: ''
#node_exporter_container_image: "docker.io/prom/node-exporter:v0.17.0"
#node_exporter_port: 9100
grafana_admin_user: admin
# This variable must be set with a strong custom password when dashboard_enabled is True
grafana_admin_password: admin
# We only need this for SSL (https) connections
#grafana_crt: ''
#grafana_key: ''
# When using https, please fill with a hostname for which grafana_crt is valid.
#grafana_server_fqdn: ''
#grafana_container_image: "docker.io/grafana/grafana:5.4.3"
#grafana_container_cpu_period: 100000
#grafana_container_cpu_cores: 2
# container_memory is in GB
#grafana_container_memory: 4
#grafana_uid: 472
#grafana_datasource: Dashboard
#grafana_dashboards_path: "/etc/grafana/dashboards/ceph-dashboard"
#grafana_dashboard_version: master
#grafana_dashboard_files:
#  - ceph-cluster.json
#  - cephfs-overview.json
#  - host-details.json
#  - hosts-overview.json
#  - osd-device-details.json
#  - osds-overview.json
#  - pool-detail.json
#  - pool-overview.json
#  - radosgw-detail.json
#  - radosgw-overview.json
#  - rbd-overview.json
#grafana_plugins:
#  - vonage-status-panel
#  - grafana-piechart-panel
#grafana_allow_embedding: True
#grafana_port: 3000
#prometheus_container_image: "docker.io/prom/prometheus:v2.7.2"
#prometheus_container_cpu_period: 100000
#prometheus_container_cpu_cores: 2
# container_memory is in GB
#prometheus_container_memory: 4
#prometheus_data_dir: /var/lib/prometheus
#prometheus_conf_dir: /etc/prometheus
#prometheus_user_id: '65534'  # This is the UID used by the prom/prometheus container image
#prometheus_port: 9092
#alertmanager_container_image: "docker.io/prom/alertmanager:v0.16.2"
#alertmanager_container_cpu_period: 100000
#alertmanager_container_cpu_cores: 2
# container_memory is in GB
#alertmanager_container_memory: 4
#alertmanager_data_dir: /var/lib/alertmanager
#alertmanager_conf_dir: /etc/alertmanager
#alertmanager_port: 9093
#alertmanager_cluster_port: 9094


##################################
# DEPRECIATED iSCSI TARGET SETUP #
##################################

# WARNING #

# The following values are depreciated. To setup targets, gateways, LUNs, and
# clients you should use gwcli or dashboard. If the following values are set,
# the old ceph-iscsi-config/ceph-iscsi-cli packages will be used.

# Specify the iqn for ALL gateways. This iqn is shared across the gateways, so an iscsi
# client sees the gateway group as a single storage subsystem.
#gateway_iqn: ""

# gateway_ip_list provides a list of the IP Addrresses - one per gateway - that will be used
# as an iscsi target portal ip. The list must be comma separated - and the order determines
# the sequence of TPG's within the iscsi target across each gateway. Once set, additional
# gateways can be added, but the order must *not* be changed.
#gateway_ip_list: 0.0.0.0

# rbd_devices defines the images that should be created and exported from the iscsi gateways.
# If the rbd does not exist, it will be created for you. In addition you may increase the
# size of rbd's by changing the size parameter and rerunning the playbook. A size value lower
# than the current size of the rbd is ignored.
#
# the 'host' parameter defines which of the gateway nodes should handle the physical
# allocation/expansion or removal of the rbd
# to remove an image, simply use a state of 'absent'. This will first check the rbd is not allocated
# to any client, and the remove it from LIO and then delete the rbd image
#
# NB. this variable definition can be commented out to bypass LUN management
#
# Example:
#
#rbd_devices:
#  - { pool: 'rbd', image: 'ansible1', size: '30G', host: 'ceph-1', state: 'present' }
#  - { pool: 'rbd', image: 'ansible2', size: '15G', host: 'ceph-1', state: 'present' }
#  - { pool: 'rbd', image: 'ansible3', size: '30G', host: 'ceph-1', state: 'present' }
#  - { pool: 'rbd', image: 'ansible4', size: '50G', host: 'ceph-1', state: 'present' }
#rbd_devices: {}

# client_connections defines the client ACL's to restrict client access to specific LUNs
# The settings are as follows;
# - image_list is a comma separated list of rbd images of the form <pool name>.<rbd_image_name>
# - chap supplies the user and password the client will use for authentication of the
#   form <user>/<password>
# - status shows the intended state of this client definition - 'present' or 'absent'
#
# NB. this definition can be commented out to skip client (nodeACL) management
#
# Example:
#
#client_connections:
#  - { client: 'iqn.1994-05.com.redhat:rh7-iscsi-client', image_list: 'rbd.ansible1,rbd.ansible2', chap: 'rh7-iscsi-client/redhat', status: 'present' }
#  - { client: 'iqn.1991-05.com.microsoft:w2k12r2', image_list: 'rbd.ansible4', chap: 'w2k12r2/microsoft_w2k12', status: 'absent' }

#client_connections: {}



###############
# DEPRECATION #
###############



######################################################
# VARIABLES BELOW SHOULD NOT BE MODIFIED BY THE USER #
#               *DO NOT* MODIFY THEM                 #
######################################################

#container_exec_cmd:
#docker: false
#ceph_volume_debug: "{{ enable_ceph_volume_debug | ternary(1, 0)  }}"
