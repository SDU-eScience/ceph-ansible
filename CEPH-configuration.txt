ceph config set global mon_target_pg_per_osd 100
ceph config set global osd_pool_default_pg_autoscale_mode on

ceph osd erasure-code-profile set ec_83h k=8 m=3 crush-failure-domain=host plugin=jerasure technique=reed_sol_van
ceph osd pool create cephfs_data 1024 1024 erasure ec_83h
ceph osd pool set cephfs_data target_size_ratio .5
ceph osd pool set cephfs_data allow_ec_overwrites true
ceph osd pool create cephfs_metadata 512 512 replicated
ceph osd pool set cephfs_metadata target_size_ratio .05

#ceph osd pool set cephfs_data pg_num 1024

# increase recovery speed //these are ephemeral
# default 1
ceph tell 'osd.*' config set osd_max_backfills 8
# default 3
ceph tell 'osd.*' config set osd_recovery_max_active 4

# ceph k8s
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

# ceph device health prediction
ceph device monitoring on
ceph config set global device_failure_prediction_mode local

# ceph balancer
ceph osd set-require-min-compat-client luminous
ceph balancer mode upmap
ceph balancer on

# to pause the CEPH cluster
ceph osd set noout
ceph osd set nobackfill
ceph osd set norecover
ceph osd set norebalance
ceph osd set nodown
ceph osd set pause
# the first 3 above 
#Those flags should be totally sufficient to safely powerdown your cluster but you
#could also set the following flags on top if you would like to pause your cluster completely

ceph osd unset pause
ceph osd unset nodown
ceph osd unset norebalance
ceph osd unset norecover
ceph osd unset nobackfill
ceph osd unset noout


# dashboard
yum install ceph-mgr-dashboard
ceph mgr module enable dashboard
ceph dashboard create-self-signed-cert
firewall-cmd --permanent --add-port=8443/tcp
ceph dashboard ac-user-create user pass administrator



# notes
# to be able to remove a pool
ceph tell mon.\* injectargs '--mon-allow-pool-delete=true'
ceph osd pool delete testbench testbench --yes-i-really-really-mean-it

# to mount a rbd with the kernel driver
rbd map test3 -p escience --key AQAVoa1c4iKUKhAAf4ulmQ2c2p/i/3rk2MuFBg== --id manager1 -m 172.26.3.1:6789

# to mount a rbd with the rbd-ndb / nbd driver + librbd usersapce lib
rbd-nbd map escience/test4 --key AQAVoa1c4iKUKhAAf4ulmQ2c2p/i/3rk2MuFBg== --id manager1 -m 172.26.3.1:6789 [--device /dev/nbd4]


###############################
# enable NAT ## to delete the rule change -A with -D 
# to list all rules -S 
# -t NAT select the NAT table 
iptables -t nat -A POSTROUTING -o p3p1.99 -j MASQUERADE
iptables -A FORWARD -i p3p1.99 -o p3p1 -m state --state RELATED,ESTABLISHED -j ACCEPT
iptables -A FORWARD -i p3p1 -o p3p1.99 -j ACCEPT
sysctl -w net.ipv4.ip_forward=1

## configure docker registry
* set dns name dockerregistry.cloud.sdu.docker
* configure the regitry
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce-18.06.3.ce-3.el7
yum install -y containerd.io
systemctl enable docker
systemctl start docker
docker run -d -p 172.16.1.1:5000:5000 --name registry -v /mnt/docker_repo:/var/lib/registry --restart=unless-stopped registry:2


################
# PG per osd
################
ceph pg dump | awk '
BEGIN { IGNORECASE = 1 }
 /^PG_STAT/ { col=1; while($col!="UP") {col++}; col++ }
 /^[0-9a-f]+\.[0-9a-f]+/ { match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;
 up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)>0) { osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) }
 for(i in osds) {array[osds[i],pool]++; osdlist[osds[i]];}
}
END {
 printf("\n");
 printf("pool :\t"); for (i in poollist) printf("%s\t",i); printf("| SUM \n");
 for (i in poollist) printf("--------"); printf("----------------\n");
 for (i in osdlist) { printf("osd.%i\t", i); sum=0;
   for (j in poollist) { printf("%i\t", array[i,j]); sum+=array[i,j]; sumpool[j]+=array[i,j] }; printf("| %i\n",sum) }
 for (i in poollist) printf("--------"); printf("----------------\n");
 printf("SUM :\t"); for (i in poollist) printf("%s\t",sumpool[i]); printf("|\n");
}'

# rados bench bandw
rados bench -p test_ec 20 write --no-cleanup -t 32 --id test --run-name `hostname`
rados bench -p test_ec 20 seq -t 32 --id test --run-name `hostname`
rados bench -p test_ec 20 rand -t 32 --id test --run-name `hostname`
rados -p test_ec cleanup --id test --run-name `hostname`

echo "##### 8+3 EC Pool"
# rados bench iops on ec pool
rados bench -p test_ec 10 write --no-cleanup -t 32 -b 32768 --id test --run-name `hostname`
rados bench -p test_ec 10 seq -t 32 --id test --run-name `hostname`
rados bench -p test_ec 10 rand -t 32 --id test --run-name `hostname`
rados -p test_ec cleanup --id test --run-name `hostname`

echo "##### 4x replicated"
# rados bench iops on replicated pool
rados bench -p escience 10 write --no-cleanup -t 32 -b 4096 --id test --run-name `hostname`
rados bench -p escience 10 seq -t 32 --id test --run-name `hostname`
rados bench -p escience 10 rand -t 32 --id test --run-name `hostname`
rados -p escience cleanup --id test --run-name `hostname`
